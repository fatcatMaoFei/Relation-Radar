#!/usr/bin/env python3
"""
Prompt Regression Test Script (PR-0.3-03)

ç”¨äºŽæµ‹è¯•å’Œå¯¹æ¯”ä¸åŒåœºæ™¯ä¸‹çš„æç¤ºè¯æ•ˆæžœã€‚
å›ºå®šä¸€ç»„å…¸åž‹ç”¨ä¾‹ï¼Œæ–¹ä¾¿äººå·¥è¯„ä¼°æç¤ºè¯è´¨é‡ã€‚

ä½¿ç”¨æ–¹æ³•:
    # æµ‹è¯•æ‰€æœ‰åœºæ™¯ï¼ˆmockæ¨¡å¼ï¼‰
    python scripts/test_prompts_regression.py
    
    # ä½¿ç”¨çœŸå®žQwenæµ‹è¯•
    RELATION_RADAR_LLM_MODE=qwen python scripts/test_prompts_regression.py
    
    # åªæµ‹è¯•ç‰¹å®šåœºæ™¯
    python scripts/test_prompts_regression.py --scenario gift
"""
from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import List, Optional

# Ensure project root is on sys.path
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from backend.llm.prompts import (  # noqa: E402
    build_qa_rag_prompt,
    build_multi_person_qa_prompt,
    build_gift_suggestion_prompt,
    build_emotion_care_prompt,
    build_person_summary_prompt,
    build_teacher_qa_prompt,
    get_prompt_version,
    get_prompt_stats,
    list_available_prompts,
)
from backend.llm.local_client import get_llm_client  # noqa: E402


# ==================== æµ‹è¯•ç”¨ä¾‹å®šä¹‰ ====================

class TestCase:
    """æµ‹è¯•ç”¨ä¾‹"""
    def __init__(
        self,
        name: str,
        scenario: str,
        prompt_builder: callable,
        prompt_args: dict,
        expected_keywords: List[str],
        avoid_keywords: List[str] = None,
        description: str = ""
    ):
        self.name = name
        self.scenario = scenario
        self.prompt_builder = prompt_builder
        self.prompt_args = prompt_args
        self.expected_keywords = expected_keywords
        self.avoid_keywords = avoid_keywords or []
        self.description = description


# æµ‹è¯•æ•°æ®ï¼šæ¨¡æ‹Ÿçš„ä¸Šä¸‹æ–‡è®°å½•
SAMPLE_CONTEXTS = {
    "cat_food": """
äº‹ä»¶1 [2025-12-01] èšé¤ï¼šå’ŒçŒ«ä¸€èµ·åƒå·èœï¼Œå¥¹å¾ˆå¼€å¿ƒï¼Œè¯´æœ€å–œæ¬¢éº»è¾£é”…åº•å’Œæ°´ç…®é±¼ï¼Œä½†ä¸å–œæ¬¢å¤ªæ²¹è…»çš„èœã€‚
äº‹ä»¶2 [2025-11-28] èŠå¤©ï¼šçŒ«æåˆ°å¥¹æœ€è¿‘åœ¨å‡è‚¥ï¼Œå°½é‡å°‘åƒç”œé£Ÿå’Œæ²¹ç‚¸é£Ÿå“ã€‚
äº‹ä»¶3 [2025-11-20] ç”Ÿæ—¥èšä¼šï¼šçŒ«çš„ç”Ÿæ—¥ï¼Œå¤§å®¶é€äº†è›‹ç³•ï¼Œå¥¹è¯´è™½ç„¶åœ¨å‡è‚¥ä½†ç”Ÿæ—¥å¯ä»¥ä¾‹å¤–ã€‚
""",
    "cat_emotion": """
äº‹ä»¶1 [2025-12-08] èŠå¤©ï¼šçŒ«è¯´æœ€è¿‘å·¥ä½œåŽ‹åŠ›å¾ˆå¤§ï¼Œç»å¸¸åŠ ç­åˆ°å¾ˆæ™šï¼Œæœ‰äº›ç„¦è™‘ã€‚
äº‹ä»¶2 [2025-12-05] å¾®ä¿¡ï¼šçŒ«å‘æ¶ˆæ¯è¯´æ„Ÿè§‰å¾ˆç´¯ï¼Œæƒ³æ‰¾æ—¶é—´æ”¾æ¾ä¸€ä¸‹ã€‚
äº‹ä»¶3 [2025-12-01] èšé¤ï¼šè™½ç„¶å·¥ä½œåŽ‹åŠ›å¤§ï¼Œä½†å’Œæœ‹å‹èšé¤æ—¶å¥¹è¿˜æ˜¯å¾ˆå¼€å¿ƒçš„ã€‚
""",
    "multi_person": """
ã€å…³äºŽçŒ«çš„è®°å½•ã€‘
äº‹ä»¶1ï¼šçŒ«å–œæ¬¢åƒå·èœå’Œéº»è¾£å£å‘³ï¼Œä¸å–œæ¬¢å¤ªæ²¹è…»çš„é£Ÿç‰©ã€‚
äº‹ä»¶2ï¼šçŒ«æœ€è¿‘åœ¨å‡è‚¥ï¼Œå°½é‡å°‘åƒç”œé£Ÿã€‚

ã€å…³äºŽé˜¿Bçš„è®°å½•ã€‘
äº‹ä»¶1ï¼šé˜¿Bå–œæ¬¢æ¸…æ·¡å£å‘³ï¼Œä¸å¤ªèƒ½åƒè¾£ã€‚
äº‹ä»¶2ï¼šé˜¿Bå¯¹æµ·é²œè¿‡æ•ï¼Œä¸èƒ½åƒè™¾èŸ¹ã€‚
""",
    "gift_context": """
äº‹ä»¶1ï¼šçŒ«å–œæ¬¢çœ‹ä¹¦ï¼Œæœ€è¿‘åœ¨è¯»å¿ƒç†å­¦ç›¸å…³çš„ä¹¦ç±ã€‚
äº‹ä»¶2ï¼šçŒ«å–œæ¬¢å®‰é™çš„çŽ¯å¢ƒï¼Œå‘¨æœ«ç»å¸¸åŽ»å’–å•¡é¦†ã€‚
äº‹ä»¶3ï¼šçŒ«æåˆ°æƒ³å­¦ç‘œä¼½æ”¾æ¾èº«å¿ƒã€‚
äº‹ä»¶4ï¼šçŒ«ä¸å–œæ¬¢å¤ªèŠ±å“¨çš„ä¸œè¥¿ï¼Œåå¥½ç®€çº¦é£Žæ ¼ã€‚
""",
}

# å®šä¹‰æµ‹è¯•ç”¨ä¾‹
TEST_CASES = [
    # åœºæ™¯1ï¼šé¥®é£Ÿåå¥½é—®ç­”
    TestCase(
        name="é¥®é£Ÿåå¥½æŸ¥è¯¢",
        scenario="food",
        prompt_builder=build_qa_rag_prompt,
        prompt_args={
            "question": "çŒ«å–œæ¬¢åƒä»€ä¹ˆï¼Ÿæœ‰ä»€ä¹ˆå¿Œå£ï¼Ÿ",
            "context": SAMPLE_CONTEXTS["cat_food"]
        },
        expected_keywords=["éº»è¾£", "å·èœ", "ä¸å–œæ¬¢", "æ²¹è…»", "å‡è‚¥"],
        avoid_keywords=["æˆ‘è®¤ä¸º", "å¯èƒ½å–œæ¬¢"],
        description="æµ‹è¯•åŸºäºŽè®°å½•çš„é¥®é£Ÿåå¥½å›žç­”"
    ),
    
    # åœºæ™¯2ï¼šé€ç¤¼å»ºè®®
    TestCase(
        name="ç”Ÿæ—¥é€ç¤¼å»ºè®®",
        scenario="gift",
        prompt_builder=build_gift_suggestion_prompt,
        prompt_args={
            "person_name": "çŒ«",
            "context": SAMPLE_CONTEXTS["gift_context"],
            "occasion": "ç”Ÿæ—¥",
            "budget": "200-500å…ƒ"
        },
        expected_keywords=["ä¹¦", "ç‘œä¼½", "ç®€çº¦"],
        avoid_keywords=["èŠ±å“¨", "ä¸ç¡®å®š"],
        description="æµ‹è¯•åŸºäºŽåå¥½çš„ç¤¼ç‰©æŽ¨è"
    ),
    
    # åœºæ™¯3ï¼šæƒ…ç»ªå…³æ€€
    TestCase(
        name="æƒ…ç»ªå…³æ€€å»ºè®®",
        scenario="emotion",
        prompt_builder=build_emotion_care_prompt,
        prompt_args={
            "person_name": "çŒ«",
            "context": SAMPLE_CONTEXTS["cat_emotion"],
            "recent_emotion": "ç„¦è™‘"
        },
        expected_keywords=["åŽ‹åŠ›", "æ”¾æ¾", "å…³å¿ƒ"],
        avoid_keywords=["æŠ‘éƒç—‡", "å¿ƒç†æ²»ç–—"],
        description="æµ‹è¯•æƒ…ç»ªå…³æ€€å»ºè®®çš„æ¸©å’Œåº¦"
    ),
    
    # åœºæ™¯4ï¼šå¤šäººåœºæ™¯
    TestCase(
        name="å¤šäººèšé¤å»ºè®®",
        scenario="multi",
        prompt_builder=build_multi_person_qa_prompt,
        prompt_args={
            "question": "æƒ³çº¦çŒ«å’Œé˜¿Bä¸€èµ·åƒé¥­ï¼ŒåŽ»ä»€ä¹ˆé¤åŽ…æ¯”è¾ƒåˆé€‚ï¼Ÿ",
            "context": SAMPLE_CONTEXTS["multi_person"],
            "person_names": ["çŒ«", "é˜¿B"]
        },
        expected_keywords=["å†²çª", "è¾£", "æ¸…æ·¡", "æµ·é²œ"],
        avoid_keywords=[],
        description="æµ‹è¯•å¤šäººéœ€æ±‚å¹³è¡¡"
    ),
    
    # åœºæ™¯5ï¼šäººç‰©ç”»åƒ
    TestCase(
        name="äººç‰©ç”»åƒç”Ÿæˆ",
        scenario="summary",
        prompt_builder=build_person_summary_prompt,
        prompt_args={
            "person_name": "çŒ«",
            "events_summary": "æœ€è¿‘å’ŒçŒ«èšé¤äº†3æ¬¡ï¼Œå¥¹éƒ½é€‰æ‹©å·èœé¦†ã€‚å·¥ä½œæ¯”è¾ƒå¿™ï¼Œå¶å°”ä¼šæŠ±æ€¨åŽ‹åŠ›å¤§ã€‚",
            "preferences": ["éº»è¾£å£å‘³", "å®‰é™çŽ¯å¢ƒ", "é˜…è¯»"],
            "taboos": ["æ²¹è…»é£Ÿç‰©", "ç”œé£Ÿï¼ˆå‡è‚¥ä¸­ï¼‰"]
        },
        expected_keywords=["å·èœ", "åŽ‹åŠ›", "å®‰é™"],
        avoid_keywords=["æˆ‘çŒœ", "åº”è¯¥æ˜¯"],
        description="æµ‹è¯•äººç‰©ç”»åƒçš„å‡†ç¡®æ€§"
    ),
    
    # åœºæ™¯6ï¼šTeacheré—®ç­”
    TestCase(
        name="Teacherä¸“ä¸šå›žç­”",
        scenario="teacher",
        prompt_builder=build_teacher_qa_prompt,
        prompt_args={
            "question": "çŒ«æœ€è¿‘å¿ƒæƒ…ä¸å¥½ï¼Œæˆ‘åº”è¯¥æ€Žä¹ˆå…³å¿ƒå¥¹ï¼Ÿ",
            "facts": "çŒ«æœ€è¿‘å·¥ä½œåŽ‹åŠ›å¤§ï¼Œç»å¸¸åŠ ç­ï¼Œæœ‰ç„¦è™‘æƒ…ç»ªã€‚å¥¹å–œæ¬¢å®‰é™çš„çŽ¯å¢ƒå’Œé˜…è¯»ã€‚",
            "local_answer": "å¯ä»¥çº¦å¥¹å‡ºæ¥èŠèŠå¤©ï¼Œå¬å¥¹å€¾è¯‰ã€‚"
        },
        expected_keywords=["åŽ‹åŠ›", "å…³å¿ƒ", "å€¾å¬"],
        avoid_keywords=["æŠ‘éƒ", "çœ‹åŒ»ç”Ÿ"],
        description="æµ‹è¯•Teacheræ¨¡åž‹çš„ä¸“ä¸šåº¦"
    ),
]


# ==================== æµ‹è¯•æ‰§è¡Œ ====================

def run_test_case(test_case: TestCase, client) -> dict:
    """æ‰§è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
    # æž„å»ºprompt
    prompt = test_case.prompt_builder(**test_case.prompt_args)
    
    # è°ƒç”¨LLM
    response = client.generate(prompt, max_tokens=512)
    
    # æ£€æŸ¥å…³é”®è¯
    found_expected = [kw for kw in test_case.expected_keywords if kw in response]
    found_avoid = [kw for kw in test_case.avoid_keywords if kw in response]
    
    # è®¡ç®—å¾—åˆ†
    expected_score = len(found_expected) / len(test_case.expected_keywords) if test_case.expected_keywords else 1.0
    avoid_penalty = len(found_avoid) / len(test_case.avoid_keywords) if test_case.avoid_keywords else 0.0
    final_score = max(0, expected_score - avoid_penalty * 0.5)
    
    return {
        "name": test_case.name,
        "scenario": test_case.scenario,
        "description": test_case.description,
        "prompt_preview": prompt[:200] + "..." if len(prompt) > 200 else prompt,
        "response": response,
        "expected_keywords": test_case.expected_keywords,
        "found_expected": found_expected,
        "avoid_keywords": test_case.avoid_keywords,
        "found_avoid": found_avoid,
        "score": final_score,
    }


def print_result(result: dict, verbose: bool = False):
    """æ‰“å°æµ‹è¯•ç»“æžœ"""
    score = result["score"]
    if score >= 0.8:
        status = "âœ…"
    elif score >= 0.5:
        status = "âš ï¸"
    else:
        status = "âŒ"
    
    print(f"\n{'='*60}")
    print(f"{status} {result['name']} (åœºæ™¯: {result['scenario']})")
    print(f"{'='*60}")
    print(f"ðŸ“ {result['description']}")
    print(f"ðŸ“Š å¾—åˆ†: {score:.0%}")
    print(f"   - æœŸæœ›å…³é”®è¯: {result['expected_keywords']}")
    print(f"   - æ‰¾åˆ°: {result['found_expected']}")
    if result["avoid_keywords"]:
        print(f"   - åº”é¿å…: {result['avoid_keywords']}")
        if result["found_avoid"]:
            print(f"   - âš ï¸ å‘çŽ°åº”é¿å…çš„: {result['found_avoid']}")
    
    print("\nðŸ¤– AIå›žç­”:")
    print("-" * 40)
    # æˆªæ–­è¿‡é•¿çš„å›žç­”
    response = result["response"]
    if len(response) > 500:
        print(response[:500] + "...")
    else:
        print(response)
    print("-" * 40)
    
    if verbose:
        print("\nðŸ“œ Prompté¢„è§ˆ:")
        print(result["prompt_preview"])


def run_all_tests(
    scenarios: Optional[List[str]] = None,
    verbose: bool = False
) -> dict:
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ðŸš€ Promptå›žå½’æµ‹è¯• (PR-0.3-03)")
    print(f"ðŸ“Œ æç¤ºè¯ç‰ˆæœ¬: {get_prompt_version()}")
    print(f"ðŸ“ å¯ç”¨æ¨¡æ¿: {list_available_prompts()}")
    
    # èŽ·å–LLMå®¢æˆ·ç«¯
    client = get_llm_client()
    print(f"ðŸ¤– LLMæ¨¡å¼: {client._mode}")
    
    # ç­›é€‰æµ‹è¯•ç”¨ä¾‹
    if scenarios:
        test_cases = [tc for tc in TEST_CASES if tc.scenario in scenarios]
    else:
        test_cases = TEST_CASES
    
    print(f"\nðŸ“‹ å°†è¿è¡Œ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    
    # æ‰§è¡Œæµ‹è¯•
    results = []
    for tc in test_cases:
        try:
            result = run_test_case(tc, client)
            results.append(result)
            print_result(result, verbose)
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥: {tc.name}")
            print(f"   é”™è¯¯: {e}")
            results.append({
                "name": tc.name,
                "scenario": tc.scenario,
                "score": 0,
                "error": str(e)
            })
    
    # æ±‡æ€»ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ðŸ“Š æµ‹è¯•æ±‡æ€»")
    print("=" * 60)
    
    total_score = sum(r.get("score", 0) for r in results) / len(results) if results else 0
    passed = sum(1 for r in results if r.get("score", 0) >= 0.8)
    warned = sum(1 for r in results if 0.5 <= r.get("score", 0) < 0.8)
    failed = sum(1 for r in results if r.get("score", 0) < 0.5)
    
    print(f"æ€»ä½“å¾—åˆ†: {total_score:.0%}")
    print(f"âœ… é€šè¿‡: {passed}")
    print(f"âš ï¸ è­¦å‘Š: {warned}")
    print(f"âŒ å¤±è´¥: {failed}")
    
    # æŒ‰åœºæ™¯ç»Ÿè®¡
    print("\næŒ‰åœºæ™¯ç»Ÿè®¡:")
    scenarios_scores = {}
    for r in results:
        s = r["scenario"]
        if s not in scenarios_scores:
            scenarios_scores[s] = []
        scenarios_scores[s].append(r.get("score", 0))
    
    for s, scores in scenarios_scores.items():
        avg = sum(scores) / len(scores)
        print(f"  - {s}: {avg:.0%}")
    
    return {
        "version": get_prompt_version(),
        "total_score": total_score,
        "passed": passed,
        "warned": warned,
        "failed": failed,
        "results": results
    }


def show_prompt_stats():
    """æ˜¾ç¤ºæç¤ºè¯ç»Ÿè®¡ä¿¡æ¯"""
    stats = get_prompt_stats()
    print("\nðŸ“Š æç¤ºè¯æ¨¡æ¿ç»Ÿè®¡")
    print("=" * 60)
    print(f"ç‰ˆæœ¬: {stats['version']}")
    print("\næ¨¡æ¿è¯¦æƒ…:")
    for name, info in stats["templates"].items():
        placeholder = "âœ“" if info["has_placeholders"] else "âœ—"
        print(f"  - {name}: {info['lines']}è¡Œ, {info['size']}å­—èŠ‚, å ä½ç¬¦:{placeholder}")


def main():
    parser = argparse.ArgumentParser(description="Promptå›žå½’æµ‹è¯•")
    parser.add_argument(
        "--scenario", "-s",
        choices=["food", "gift", "emotion", "multi", "summary", "teacher"],
        help="åªæµ‹è¯•ç‰¹å®šåœºæ™¯"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬promptï¼‰"
    )
    parser.add_argument(
        "--stats",
        action="store_true",
        help="æ˜¾ç¤ºæç¤ºè¯ç»Ÿè®¡ä¿¡æ¯"
    )
    
    args = parser.parse_args()
    
    if args.stats:
        show_prompt_stats()
        return
    
    scenarios = [args.scenario] if args.scenario else None
    results = run_all_tests(scenarios=scenarios, verbose=args.verbose)
    
    # è¿”å›žç 
    if results["failed"] > 0:
        sys.exit(1)
    sys.exit(0)


if __name__ == "__main__":
    main()
